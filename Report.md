# 实验报告

以下小标题每一项分别为：activation functions + loss functions
标题下第一行分别为：training loss + training accuracy + test loss + test accuracy

## 单隐藏层

learning rate + weight decay + momentum为$0.01$+$0.0001$+$0.9$，隐藏维数为$28$

1. `Selu`+`SoftmaxCrossEntropy`: 

	$0.0398$ $0.9858$ $0.11026$ $0.96670$
	
	![1](.\1.png)

2. `HardSwish`+`SoftmaxCrossEntropy`: 
	
	$0.0692$ $0.9770$ $0.13953$ $0.96480$
	
	![2](.\2.png)

3. `Tanh`+`SoftmaxCrossEntropy`: 
	
	$0.0548$ $0.9850$ $0.11807$ $0.96560$
	
	![3](.\3.png)

4. `Selu`+`KLDivLoss`: 
	
	$0.0364$ $0.9892$ $0.10163$ $0.97150$
	
	![4](.\4.png)
	
5. `Selu`+`HingeLoss`: without tuning
	
	$0.1973$ $0.9868$ $0.72691$ $0.96940$
	
	![5](.\5.png)

6. `HardSwish`+`KLDivLoss`: 
	
	$0.0805$ $0.9732$ $0.15498$ $0.96000$
	
	![6](.\6.png)

7. `HardSwish`+`HingeLoss`: without tuning
	
	$0.3964$ $0.9770$ $1.09784$ $0.96100$
	
	![7](.\7.png)

8. `Tanh`+`KLDivLoss`: 
	
	$0.0472$ $0.9872$ $0.10991$ $0.96750$
	
	![8](.\8.png)

9. `Tanh`+`HingeLoss`: without tuning
	
	$0.3205$ $0.9820$ $0.92683$ $0.95360$
	
	![9](.\9.png)

### Discussion

1. Training time：无明显区别，但`HingeLoss`加了`Softmax`后更慢
2. Convergence：观察测试集，`HardSwish`在三种激活函数中相对收敛更慢，`HingeLoss`在三种损失函数中相对收敛更快（不平滑）；总体收敛性均较好，但`HardSwish`和`HingeLoss`会有波动
3. Accuracy：对于训练集，`HardSwish`的准确率明显低于其他二者，而三种损失函数表现相当；对于测试集，`Selu`的准确率比较出色而`HardSwish`相反，`HingeLoss`较差；特别地，`Tanh`和`HingeLoss`的组合效果不好

总结：`HardSwish`和`HingeLoss`表现不太稳定，`Selu`则表现良好；`SoftmaxCrossEntropy`和`KLDivLoss`由于只差了一个常数，表现十分相似

注：以上`HingeLoss`均使用$\Delta = 5$，不加`Softmax`

## Tuning HingeLoss

`HingeLoss`意图让正确维度比错误维度大$\Delta$，与绝对值有关，最好进行控制，并调整$\Delta$。以下采用上述实验中表现最好的`Selu`作为激活函数，learning rate + weight decay + momentum为$0.01$+$0.0001$+$0.9$，隐藏维数为$28$

1. `Selu`+`HingeLoss`+$0.01$+$0.0001$+$0.9$: $\Delta = 0.5$ + `Softmax`
	
	$0.0454$ $0.9878$ $0.13729$ $0.96600$
	
	![10](.\10.png)

注意到以上结果中仍然收敛不平滑且波动较大，试着将$\Delta$调到更大以避免梯度消失

2. `Selu`+`HingeLoss`+$0.01$+$0.0001$+$0.9$: $\Delta = 1$ + `Softmax`
	
	$0.2056$ $0.9812$ $0.31461$ $0.96980$
	
	![11](.\11.png)
	

这下我们取得了较好的结果，但综合三个激活函数，最终选择1作为后续实验的参数

## Bonus: Hyperparameters

我们选用以上实验中表现中规中矩的`Tanh`和`SoftmaxCrossEntropy`组合进行调参

### Learning Rate

$0.01$：

$0.0486$ $0.9864$ $0.11249$ $0.96600$

![20](.\20.png)

可以看到，loss收敛稍快，且测试集上的表现与训练集有差距，可能略微有些过拟合

$0.003$：

$0.0761$ $0.9774$ $0.11448$ $0.96540$

![21](.\21.png)

训练集上表现稍差，但测试集上表现相当，说明泛化能力略有增强，后续采用

$0.0005$：

$0.1860$ $0.9494$ $0.18763$ $0.94450$

![22](.\22.png)

训练集和测试集表现相差无几，都较差，训练不彻底

#### Analysis

学习率过大时，训练时收敛快效果好，但会导致过拟合使泛化能力下降；过小时泛化能力虽强，但训练慢，总体能力差；适中时才能兼顾泛化能力和任务能力

### Weight Decay

$0.0001$：

$0.0761$ $0.9774$ $0.11448$ $0.96540$

![21](.\21.png)

$0.001$：

$0.1380$ $0.9634$ $0.14729$ $0.95770$

![23](.\23.png)

在训练集和测试集上，loss均较高，表现较差

$0.0003$：

$0.0943$ $0.9740$ $0.11904$ $0.96200$

![25](.\25.png)

$0.00001$：

$0.0848$ $0.9784$ $0.11899$ $0.96450$

![24](.\24.png)

训练集acc比第1组好，但测试集反而不如，泛化能力弱

最终$0.0001$效果最好，后续采用

#### Analysis

权重衰减有助于模型学习到更简单的权重分布，从而提高泛化能力。过大时学习慢，loss高，无法有效学习，导致欠拟合；过小时拟合过度，实际效果不好

### Momentum

$0.9$：

$0.0761$ $0.9774$ $0.11448$ $0.96540$

![21](.\21.png)

$0.999$：

$0.3621$ $0.9014$ $0.44651$ $0.89490$

![26](.\26.png)

振荡过于剧烈，收敛效果不好

$0.5$：

$0.0778$ $0.9770$ $0.11783$ $0.96590$

![27](.\27.png)

$0$：

$0.2295$ $0.9330$ $0.23119$ $0.93320$

![28](.\28.png)

收敛过慢，loss过大，可能陷入了局部最优

最终$0.9$最优，后续采用

#### Analysis

动量通过使用之前的梯度来加速梯度下降。过大时过于依赖之前的梯度，导致更新幅度过大，难以收敛到最优解；过小时仅依赖当前梯度，容易产生“椭圆”情况训练缓慢或陷入局部最小值点

#### Batch Size

$100$：

$0.0761$ $0.9774$ $0.11448$ $0.96540$

![21](.\21.png)

$10$：

$0.0736$ $0.9880$ $0.13776$ $0.95940$

![29](.\29.png)

训练开销降低，但loss和acc均波动明显，效果不好

$1000$：

$0.2249$ $0.9357$ $0.23001$ $0.93440$

![30](.\30.png)

训练比较平滑，但更新次数少，最终效果不好

$100$最优，后续采用

#### Analysis

批量主要影响梯度计算。过大时，虽然梯度准确，但开销大，而且轮数相同时更新次数更少，结果不一定更好；过小时，梯度无法反应函数的真实情况，导致较大波动

## Two-layers

这里选择前面调试好的经典组合`Tanh`+`SoftmaxCrossEntropy`与表现最差的`Tanh`+`HingeLoss`进行实验，隐藏维数为$112$和$28$

1. `Tanh`+`SoftmaxCrossEntropy`: 
	
	$0.0178$ $0.9970$ $0.07390$ $0.97760$
	
	![40](.\40.png)
	
2. `Tanh`+`HingeLoss`:

	$0.0089$ $0.9988$ $0.08365$ $0.97720$
	
	![41](.\41.png)
	

无论是测试集还是训练集，二者的loss、acc和收敛性明显都比单层的效果好很多，但随之而来的是训练开销的大幅增大。对于单隐藏层结构，这个任务稍微有点复杂，导致模型无法准确地拟合；增加一层后，模型变得更复杂，对函数的拟合更好。但与此同时，更精确的拟合可能会带来泛化能力的下降，想要更上一层楼还需要对超参数的进一步调整

## Bonus: FocalLoss

单隐藏层（$28$）+`Tanh`：

$0.0120$ $0.9330$ $0.01235$ $0.93020$

![50](.\50.png)

测试集表现和训练集相当，可能是学习率太小，调到$0.03$：

$0.0062$ $0.9644$ $0.00673$ $0.95860$

![51](.\51.png)

再把动量调到$0.99$：

$0.0062$ $0.9614$ $0.00641$ $0.96150$

![52](.\52.png)

虽然有些振荡，但是最后结果还不错

最后使用双隐藏层（$112$ $28$），动量调回$0.9$：

$0.0030$ $0.9800$ $0.00444$ $0.97110$

![53](.\53.png)

### 对比`SoftmaxCrossEntropy`

1. Training time：更长，可能是因为梯度计算更加复杂
2. Convergence：超参数相同情况下，收敛速度大幅减慢，起始阶段最为明显
3. Accuracy：超参数相同情况下，收敛慢训练不充分，明显更差

Reason：超参数相同时`FocalLoss`各项数据不如`SoftmaxCrossEntropy`，调高学习率和动量后有所好转，很可能是因为梯度过小，更新太小，学习不充分且容易陷入局部最小值点。对比来看，由于梯度偏小，`FocalLoss`可能更适合于特征维数更大，需要更长训练时间的更复杂的神经网络和任务。同时，`FocalLoss`增加了两个因子$\alpha$和$\gamma$，可以调控正负样本和难易样本的权重，对于复杂数据集有更好的鲁棒性，但在好的数据集上表现相对弱

## Bonus：Stability

1. 梯度消失：`HingeLoss`和`HardSwish`会有梯度突然消失的情况，可能是二者振荡较大的原因；`Tanh`在输入绝对值很大的时候梯度接近于$0$，可能是`Tanh`和`HingeLoss`组合效果不佳的原因
2. 数值溢出：某些时刻`Tanh`计算exp和损失函数计算log时会溢出
3. 超参数：学习率和动量过大会放大梯度，批量过小会导致梯度不准，都可能使计算不稳定